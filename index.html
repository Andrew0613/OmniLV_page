<!doctype html>
<html lang="en">
  <head>
    <title>Lumina-OmniLV</title>
    <link rel="icon" type="image/png" href="./static/img/omnilv_teaser.png">

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Lumina-OmniLV is a unified multimodal framework for general low-level vision, covering 100+ sub-tasks including restoration, enhancement, dense prediction, and stylization.">

    <meta property="og:url" content="https://Andrew0613.github.io/OmniLV_page/">
    <meta property="og:image" content="https://Andrew0613.github.io/OmniLV_page/static/img/omnilv_teaser.png">
    <meta property="og:title" content="Lumina-OmniLV: A Unified Multimodal Framework for General Low-Level Vision">
    <meta property="og:description" content="A unified multimodal low-level vision framework with text and visual prompts.">

    <meta name="twitter:url" content="https://Andrew0613.github.io/OmniLV_page/">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="https://Andrew0613.github.io/OmniLV_page/static/img/omnilv_teaser.png">
    <meta name="twitter:title" content="Lumina-OmniLV: A Unified Multimodal Framework for General Low-Level Vision">
    <meta name="twitter:description" content="Unified multimodal framework for low-level vision with 100+ tasks.">

    <script src="./static/js/distill_template.v2.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <script defer src="./static/js/hider.js"></script>
    <script src="./static/js/image_interact.js"></script>
    <script src="./static/js/switch_videos.js"></script>

    <script defer src="./static/js/omnilv-results.js?v=20260220b"></script>

    <link rel="stylesheet" href="./static/css/style.css?v=20260220b">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Fraunces:opsz,wght@9..144,600;9..144,700&display=swap" rel="stylesheet">

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>
    <script defer src="./static/js/medium-zoom.min.js"></script>
    <script defer src="./static/js/zoom.js"></script>
  </head>
  <body>
    <div class="header-wrapper">
      <div class="header-container" id="header-container">
        <div class="header-content">
          <h1 style="margin-top: 0;"><i>Lumina-OmniLV</i></h1>
          <h2>A Unified Multimodal Framework for General Low-Level Vision</h2>

          <p>
            Lumina-OmniLV is a universal low-level vision model that supports
            <strong>100+ sub-tasks</strong> with both textual and visual prompts,
            spanning restoration, enhancement, weak-semantic dense prediction, and stylization.
          </p>

          <div class="icon-container">
            <div class="icon-item">
              <img src="./static/img/icons/visual.svg" alt="Restoration icon">
              <div><strong>Image Restoration</strong>: Unified handling of denoising, deblurring, dehazing, deraining, and artifact removal.</div>
            </div>
            <div class="icon-item">
              <img src="./static/img/icons/recipe.svg" alt="Enhancement icon">
              <div><strong>Image Enhancement</strong>: Prompt-guided brightness, color, contrast, and perceptual quality improvements.</div>
            </div>
            <div class="icon-item">
              <img src="./static/img/icons/eval.svg" alt="Dense prediction icon">
              <div><strong>Dense Prediction</strong>: A single framework for edge, depth, and normal map estimation tasks.</div>
            </div>
            <div class="icon-item">
              <img src="./static/img/icons/connector.svg" alt="Prompt interaction icon">
              <div><strong>Multimodal Prompting</strong>: Flexible interaction via text instructions and visual in-context exemplars.</div>
            </div>
          </div>

          <div class="button-container">
            <a href="https://arxiv.org/abs/2504.04903" class="button paper-link" target="_blank" rel="noopener noreferrer">
              <span class="icon is-small"><i class="ai ai-arxiv"></i></span>
              arXiv
            </a>
            <a href="https://arxiv.org/pdf/2504.04903" class="button paper-link" target="_blank" rel="noopener noreferrer">
              <span class="icon is-small"><i class="fas fa-file-pdf"></i></span>
              <span>pdf</span>
            </a>
            <a href="https://github.com/Andrew0613/Lumina-Omnilv" class="button" target="_blank" rel="noopener noreferrer">
              <span class="icon is-small"><i class="fab fa-github"></i></span>
              <span>Code</span>
            </a>
          </div>
        </div>

        <div class="header-image">
          <img draggable="false" src="./static/img/omnilv_teaser.png" alt="OmniLV teaser" class="teaser-image" data-zoomable>
        </div>
      </div>
    </div>

    <d-article>
      <div class="byline">
        <div class="byline-container">
          <p>
            <a class="author-link" href="https://andrew0613.github.io" target="_blank" rel="noopener noreferrer">Yuandong Pu<sup>1,2</sup></a> &emsp;
            <a class="author-link" href="https://le-zhuo.com" target="_blank" rel="noopener noreferrer">Le Zhuo<sup>2</sup></a> &emsp;
            <a class="author-link" href="https://scholar.google.com/citations?user=O8lP5XMAAAAJ&amp;hl=zh-CN&amp;oi=ao" target="_blank" rel="noopener noreferrer">Kaiwen Zhu<sup>1,2</sup></a> &emsp;
            <a class="author-link" href="https://liangbinxie.github.io" target="_blank" rel="noopener noreferrer">Liangbin Xie<sup>3,4</sup></a> &emsp;
            <a class="author-link" href="https://wenlongzhang0517.github.io" target="_blank" rel="noopener noreferrer">Wenlong Zhang<sup>2</sup></a> &emsp;
            <a class="author-link" href="https://chxy95.github.io" target="_blank" rel="noopener noreferrer">Xiangyu Chen<sup>2,6</sup></a> &emsp;
            <a class="author-link" href="https://scholar.google.com/citations?hl=zh-CN&amp;user=_go6DPsAAAAJ" target="_blank" rel="noopener noreferrer">Peng Gao<sup>2</sup></a> &emsp;
            <a class="author-link" href="https://mmlab.siat.ac.cn/yuqiao" target="_blank" rel="noopener noreferrer">Yu Qiao<sup>2</sup></a> &emsp;
            <a class="author-link" href="http://xpixel.group" target="_blank" rel="noopener noreferrer">Chao Dong<sup>4,5,2</sup></a> &emsp;
            <a class="author-link" href="https://lyh-18.github.io" target="_blank" rel="noopener noreferrer">Yihao Liu<sup>2,*</sup></a>
          </p>
          <p class="text" style="text-align: center; margin-bottom: 0;">
            <span class="author-note"><sup>1</sup> Shanghai Jiao Tong University &nbsp; <sup>2</sup> Shanghai AI Laboratory &nbsp; <sup>3</sup> University of Macau</span><br>
            <span class="author-note"><sup>4</sup> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences &nbsp; <sup>5</sup> Shenzhen University of Advanced Technology &nbsp; <sup>6</sup> Institute of Artificial Intelligence (TeleAI), China Telecom</span>
          </p>
          <p style="text-align: center; margin-bottom: 0;"><span class="author-note"><sup>*</sup>Corresponding author</span></p>
        </div>
      </div>

      <div class="icon-row">
        <a href="#abstract" class="icon-link">
          <img src="./static/img/icons/visual.svg" alt="Abstract icon" class="icon">Abstract
        </a>
        <a href="#method" class="icon-link">
          <img src="./static/img/icons/connector.svg" alt="Method icon" class="icon">Method
        </a>
        <a href="#results" class="icon-link">
          <img src="./static/img/icons/data.svg" alt="Results icon" class="icon">Results
        </a>
        <a href="#insights" class="icon-link">
          <img src="./static/img/icons/eval.svg" alt="Insights icon" class="icon">Insights
        </a>
        <a href="#citation" class="icon-link">
          <img src="./static/img/icons/recipe.svg" alt="Citation icon" class="icon">Citation
        </a>
      </div>

      <p class="click-hint" style="width: 85%;">
        <img src="./static/img/icons/click.gif" style="width: 1.5rem" alt="click icon">
        <strong>Click to jump to each section.</strong>
      </p>

      <hr>

      <div id="abstract" class="sub-section">
        <h1 class="text">Abstract</h1>
        <p class="text">
          We present Lumina-OmniLV (abbreviated as OmniLV), a universal multimodal multi-task framework for low-level vision that addresses over 100 sub-tasks across four major categories, including image restoration, image enhancement, weak-semantic dense prediction, and stylization. OmniLV leverages both textual and visual prompts to offer flexible, user-friendly interactions.
        </p>
        <p class="text">
          Built on Diffusion Transformer (DiT)-based generative priors, our framework supports arbitrary resolutions and achieves optimal performance at 1K resolution while preserving fine-grained details and high fidelity. Through extensive experiments, we demonstrate that separately encoding text and visual instructions, combined with co-training using shallow feature control, is essential to mitigate task ambiguity and improve multi-task generalization.
        </p>
      </div>

      <div id="method" class="sub-section">
        <h1 class="text">Method</h1>
        <figure class="method-figure">
          <img data-zoomable draggable="false" src="./static/paper_images/framework.png" alt="OmniLV framework">
          <figcaption>
            <strong>Figure 1:</strong> Overall framework of OmniLV. Input and noise latents are patchified into visual tokens, text prompts are processed by Gemma2B, and denoised latents are decoded into high-fidelity outputs.
          </figcaption>
        </figure>
      </div>

      <div id="results" class="sub-section">
        <h1 class="text">Results</h1>
        <p class="text">
          We organize results into five categories. Each category shows featured examples by default and supports one-click expansion to all pairs.
        </p>
        <div id="results-root" class="results-root" data-results-src="./static/data/omnilv-results.json">
          <p class="loading">Loading visual comparisons...</p>
        </div>
      </div>

      <div id="insights" class="sub-section">
        <h1 class="text">Insights</h1>
        <p class="text">Additional analyses from the paper highlighting model behavior and design tradeoffs.</p>

        <d-figure>
          <figure>
            <img data-zoomable draggable="false" src="./static/paper_images/mllm.png" alt="OmniLV multimodal analysis">
            <figcaption><strong>Figure 2:</strong> Multimodal prompting improves adaptability across heterogeneous low-level tasks.</figcaption>
          </figure>
        </d-figure>

        <d-figure>
          <figure>
            <img data-zoomable draggable="false" src="./static/paper_images/tsne.png" alt="Feature-space analysis">
            <figcaption><strong>Figure 3:</strong> Feature-space clustering reveals stronger task decoupling with separate text and visual encoding.</figcaption>
          </figure>
        </d-figure>

        <d-figure>
          <figure>
            <img data-zoomable draggable="false" src="./static/paper_images/mislead.png" alt="Misleading optimization analysis">
            <figcaption><strong>Figure 4:</strong> Mixing high-level generative objectives can hurt detail-sensitive restoration quality.</figcaption>
          </figure>
        </d-figure>
      </div>

      <div id="citation" class="sub-section">
        <h1 class="text">Citation</h1>
        <div class="citation-toolbar">
          <button id="copy-bibtex" type="button" class="button-inline">Copy BibTeX</button>
        </div>
        <pre><code id="bibtex-code">@article{pu2025luminaomnilv,
  title   = {Lumina-OmniLV: A Unified Multimodal Framework for General Low-Level Vision},
  author  = {Pu, Yuandong and Zhuo, Le and Zhu, Kaiwen and Xie, Liangbin and Zhang, Wenlong and Chen, Xiangyu and Gao, Peng and Qiao, Yu and Dong, Chao and Liu, Yihao},
  journal = {arXiv preprint arXiv:2504.04903},
  year    = {2025}
}</code></pre>
      </div>
    </d-article>

    <footer class="footer-note">
      <p>
        This webpage is built on the template style of Cambrian/PICABench pages and adapted for Lumina-OmniLV.
      </p>
    </footer>
  </body>
</html>
